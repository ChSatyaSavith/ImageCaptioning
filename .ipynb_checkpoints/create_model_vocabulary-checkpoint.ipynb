{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **image Preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "images_path = '../input/flickr8k-sau/Flickr_Data/Images/'\n",
    "images = glob(images_path+'*.jpg')\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    img = cv2.imread(images[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "\n",
    "incept_model = ResNet50(include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "last = incept_model.layers[-2].output\n",
    "modele = Model(inputs = incept_model.input,outputs = last)\n",
    "modele.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_features = {}\n",
    "count = 0\n",
    "for i in images:\n",
    "    img = cv2.imread(i)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    \n",
    "    img = img.reshape(1,224,224,3)\n",
    "    pred = modele.predict(img).reshape(2048,)\n",
    "        \n",
    "    img_name = i.split('/')[-1]\n",
    "    \n",
    "    images_features[img_name] = pred\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count > 1499:\n",
    "        break\n",
    "        \n",
    "    elif count % 50 == 0:\n",
    "        print(count)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_path = '../input/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = {}\n",
    "for i in captions:\n",
    "    try:\n",
    "        img_name = i.split('\\t')[0][:-2] \n",
    "        caption = i.split('\\t')[1]\n",
    "        if img_name in images_features:\n",
    "            if img_name not in captions_dict:\n",
    "                captions_dict[img_name] = [caption]\n",
    "                \n",
    "            else:\n",
    "                captions_dict[img_name].append(caption)\n",
    "            \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(captions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualize Images with captions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    img_name = images[i]\n",
    "    \n",
    "    \n",
    "    img = cv2.imread(img_name)\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.xlabel(captions_dict[img_name.split('/')[-1]])\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k in images_features.keys():\n",
    "    plt.figure()\n",
    "    \n",
    "    img_name = '../input/flickr8k-sau/Flickr_Data/Images/' + k\n",
    "    \n",
    "    \n",
    "    img = cv2.imread(img_name)\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.xlabel(captions_dict[img_name.split('/')[-1]])\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessed(txt):\n",
    "    modified = txt.lower()\n",
    "    modified = 'startofseq ' + modified + ' endofseq'\n",
    "    return modified\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in captions_dict.items():\n",
    "    for vv in v:\n",
    "        captions_dict[k][v.index(vv)] = preprocessed(vv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = {}\n",
    "for k,vv in captions_dict.items():\n",
    "    for v in vv:\n",
    "        for word in v.split():\n",
    "            if word not in count_words:\n",
    "\n",
    "                count_words[word] = 0\n",
    "\n",
    "            else:\n",
    "                count_words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = -1\n",
    "count = 1\n",
    "new_dict = {}\n",
    "for k,v in count_words.items():\n",
    "    if count_words[k] > THRESH:\n",
    "        new_dict[k] = count\n",
    "        count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict['<OUT>'] = len(new_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_backup = captions_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = captions_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, vv in captions_dict.items():\n",
    "    for v in vv:\n",
    "        encoded = []\n",
    "        for word in v.split():  \n",
    "            if word not in new_dict:\n",
    "                encoded.append(new_dict['<OUT>'])\n",
    "            else:\n",
    "                encoded.append(new_dict[word])\n",
    "\n",
    "\n",
    "        captions_dict[k][vv.index(v)] = encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build Generator Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 0\n",
    "for k, vv in captions_dict.items():\n",
    "    for v in vv:\n",
    "        if len(v) > MAX_LEN:\n",
    "            MAX_LEN = len(v)\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size = 5000\n",
    "VOCAB_SIZE = len(new_dict)\n",
    "\n",
    "def generator(photo, caption):\n",
    "    n_samples = 0\n",
    "    \n",
    "    X = []\n",
    "    y_in = []\n",
    "    y_out = []\n",
    "    \n",
    "    for k, vv in caption.items():\n",
    "        for v in vv:\n",
    "            for i in range(1, len(v)):\n",
    "                X.append(photo[k])\n",
    "\n",
    "                in_seq= [v[:i]]\n",
    "                out_seq = v[i]\n",
    "\n",
    "                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n",
    "\n",
    "                y_in.append(in_seq)\n",
    "                y_out.append(out_seq)\n",
    "            \n",
    "    return X, y_in, y_out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_in, y_out = generator(images_features, captions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(y_in), len(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y_in = np.array(y_in, dtype='float64')\n",
    "y_out = np.array(y_out, dtype='float64')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y_in.shape, y_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "max_len = MAX_LEN\n",
    "vocab_size = len(new_dict)\n",
    "\n",
    "image_model = Sequential()\n",
    "\n",
    "image_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\n",
    "image_model.add(RepeatVector(max_len))\n",
    "\n",
    "image_model.summary()\n",
    "\n",
    "language_model = Sequential()\n",
    "\n",
    "language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\n",
    "language_model.add(LSTM(256, return_sequences=True))\n",
    "language_model.add(TimeDistributed(Dense(embedding_size)))\n",
    "\n",
    "language_model.summary()\n",
    "\n",
    "conca = Concatenate()([image_model.output, language_model.output])\n",
    "x = LSTM(128, return_sequences=True)(conca)\n",
    "x = LSTM(512, return_sequences=False)(x)\n",
    "x = Dense(vocab_size)(x)\n",
    "out = Activation('softmax')(x)\n",
    "model = Model(inputs=[image_model.input, language_model.input], outputs = out)\n",
    "\n",
    "# model.load_weights(\"../input/model_weights.h5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X, y_in], y_out, batch_size=512, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_dict = {v:k for k, v in new_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('mine_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vocab.npy', new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(x):\n",
    "    \n",
    "    test_img_path = images[x]\n",
    "\n",
    "    test_img = cv2.imread(test_img_path)\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    test_img = cv2.resize(test_img, (299,299))\n",
    "\n",
    "    test_img = np.reshape(test_img, (1,299,299,3))\n",
    "    \n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    no = np.random.randint(1500,7000,(1,1))[0,0]\n",
    "    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n",
    "    \n",
    "    test_img_path = images[no]\n",
    "    test_img = cv2.imread(test_img_path)\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "    text_inp = ['startofseq']\n",
    "\n",
    "    count = 0\n",
    "    caption = ''\n",
    "    while count < 25:\n",
    "        count += 1\n",
    "\n",
    "        encoded = []\n",
    "        for i in text_inp:\n",
    "            encoded.append(new_dict[i])\n",
    "\n",
    "        encoded = [encoded]\n",
    "\n",
    "        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "\n",
    "\n",
    "        prediction = np.argmax(model.predict([test_feature, encoded]))\n",
    "\n",
    "        sampled_word = inv_dict[prediction]\n",
    "\n",
    "        caption = caption + ' ' + sampled_word\n",
    "            \n",
    "        if sampled_word == 'endofseq':\n",
    "            break\n",
    "\n",
    "        text_inp.append(sampled_word)\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.imshow(test_img)\n",
    "    plt.xlabel(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
